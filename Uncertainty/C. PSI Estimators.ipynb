{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bc498ba3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-21 23:00:27.061377: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9360] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-05-21 23:00:27.061442: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-05-21 23:00:27.061467: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1537] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-05-21 23:00:27.068637: I tensorflow/core/platform/cpu_feature_guard.cc:183] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: SSE3 SSE4.1 SSE4.2 AVX, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "/usr/local/lib/python3.10/dist-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "from src.models import mlp\n",
    "from src.datasets import load_dataset, preprocess_dataset, prefetch_dataset\n",
    "from src.psi_estimators import psi_bin_train, psi_bin_val, psi_gauss_train, psi_gauss_val, psi_neural_train, psi_neural_val, psi_rf_train, psi_rf_val\n",
    "from src.utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2e1461d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg = { 'dataset' : 'mnist',\n",
    "        'model' : 'mlp',\n",
    "        'batch_size' : 512,\n",
    "        'optimizer' : 'Adam',\n",
    "        'learning_rate' : 0.001,\n",
    "        'max_epoch' : 300,\n",
    "        'patience' : 10,}    \n",
    "\n",
    "model_name = cfg['model']\n",
    "dataset_name = cfg['dataset']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "23ba0e8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average train error: 0.09, std: 0.03\n",
      "Average validation error: 1.86, std: 0.04\n",
      "Average test error: 1.94, std: 0.05\n"
     ]
    }
   ],
   "source": [
    "##############################################################\n",
    "#\n",
    "# Compute classification error\n",
    "#\n",
    "# #############################################################\n",
    "ds_train, ds_val, ds_test, ds_info = load_dataset(cfg)\n",
    "n_classes = ds_info.features['label'].num_classes\n",
    "ds_train = preprocess_dataset(ds_train, cfg, n_classes, resize=False, normalize=True, onehot=True)\n",
    "ds_val = preprocess_dataset(ds_val, cfg, n_classes, resize=False, normalize=True, onehot=True)\n",
    "ds_test = preprocess_dataset(ds_test, cfg, n_classes, resize=False, normalize=True, onehot=True)\n",
    "ds_train = prefetch_dataset(ds_train, batch_size=cfg['batch_size'])\n",
    "ds_val = prefetch_dataset(ds_val, batch_size=cfg['batch_size'])\n",
    "ds_test = prefetch_dataset(ds_test, batch_size=cfg['batch_size'])\n",
    "\n",
    "train_acc = []\n",
    "val_acc = []\n",
    "test_acc = []\n",
    "for run in range(5):\n",
    "    tf.keras.utils.set_random_seed(run+10) # set random seed for Python, NumPy, and TensorFlow\n",
    "    model = tf.keras.models.load_model(f'../results/PI_Explainability/{model_name}_{dataset_name}/run_{run+1}/trained_model.keras')\n",
    "    train_acc.append(model.evaluate(ds_train, verbose=0)[1])\n",
    "    val_acc.append(model.evaluate(ds_val, verbose=0)[1])\n",
    "    test_acc.append(model.evaluate(ds_test, verbose=0)[1])\n",
    "print(f'Average train error: {(100-np.mean(train_acc)*100):.2f}, std: {(np.std(train_acc)*100):.2f}')\n",
    "print(f'Average validation error: {(100-np.mean(val_acc)*100):.2f}, std: {(np.std(val_acc)*100):.2f}')\n",
    "print(f'Average test error: {(100-np.mean(test_acc)*100):.2f}, std: {(np.std(test_acc)*100):.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "328cc1a9",
   "metadata": {},
   "source": [
    "### Binning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e627e640",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training PSI model (binning)...\n",
      "N_projs: 50\n",
      "Computing PSI for all validation samples and for all classes...\n",
      "Computing PSI for all test samples and for all classes...\n",
      "N_projs: 100\n",
      "Computing PSI for all validation samples and for all classes...\n",
      "Computing PSI for all test samples and for all classes...\n",
      "N_projs: 250\n",
      "Computing PSI for all validation samples and for all classes...\n",
      "Computing PSI for all test samples and for all classes...\n",
      "N_projs: 500\n",
      "Computing PSI for all validation samples and for all classes...\n",
      "Computing PSI for all test samples and for all classes...\n",
      "Training PSI model (binning)...\n",
      "N_projs: 50\n",
      "Computing PSI for all validation samples and for all classes...\n",
      "Computing PSI for all test samples and for all classes...\n",
      "N_projs: 100\n",
      "Computing PSI for all validation samples and for all classes...\n",
      "Computing PSI for all test samples and for all classes...\n",
      "N_projs: 250\n",
      "Computing PSI for all validation samples and for all classes...\n",
      "Computing PSI for all test samples and for all classes...\n",
      "N_projs: 500\n",
      "Computing PSI for all validation samples and for all classes...\n",
      "Computing PSI for all test samples and for all classes...\n",
      "Training PSI model (binning)...\n",
      "N_projs: 50\n",
      "Computing PSI for all validation samples and for all classes...\n",
      "Computing PSI for all test samples and for all classes...\n",
      "N_projs: 100\n",
      "Computing PSI for all validation samples and for all classes...\n",
      "Computing PSI for all test samples and for all classes...\n",
      "N_projs: 250\n",
      "Computing PSI for all validation samples and for all classes...\n",
      "Computing PSI for all test samples and for all classes...\n",
      "N_projs: 500\n",
      "Computing PSI for all validation samples and for all classes...\n",
      "Computing PSI for all test samples and for all classes...\n",
      "Training PSI model (binning)...\n",
      "N_projs: 50\n",
      "Computing PSI for all validation samples and for all classes...\n",
      "Computing PSI for all test samples and for all classes...\n",
      "N_projs: 100\n",
      "Computing PSI for all validation samples and for all classes...\n",
      "Computing PSI for all test samples and for all classes...\n",
      "N_projs: 250\n",
      "Computing PSI for all validation samples and for all classes...\n",
      "Computing PSI for all test samples and for all classes...\n",
      "N_projs: 500\n",
      "Computing PSI for all validation samples and for all classes...\n",
      "Computing PSI for all test samples and for all classes...\n",
      "Training PSI model (binning)...\n",
      "N_projs: 50\n",
      "Computing PSI for all validation samples and for all classes...\n",
      "Computing PSI for all test samples and for all classes...\n",
      "N_projs: 100\n",
      "Computing PSI for all validation samples and for all classes...\n",
      "Computing PSI for all test samples and for all classes...\n",
      "N_projs: 250\n",
      "Computing PSI for all validation samples and for all classes...\n",
      "Computing PSI for all test samples and for all classes...\n",
      "N_projs: 500\n",
      "Computing PSI for all validation samples and for all classes...\n",
      "Computing PSI for all test samples and for all classes...\n"
     ]
    }
   ],
   "source": [
    "for run in range(5):\n",
    "    tf.keras.utils.set_random_seed(run+10) # set random seed for Python, NumPy, and TensorFlow\n",
    "    exp_name = f'../results/PI_Explainability/{model_name}_{dataset_name}/run_{run+1}/calibration/psi/binning'\n",
    "    if not os.path.exists(exp_name):\n",
    "        print(\"Making directory\", exp_name)\n",
    "        os.makedirs(exp_name)\n",
    "\n",
    "    ds_train, ds_val, ds_test, ds_info = load_dataset(cfg)\n",
    "    n_classes = ds_info.features['label'].num_classes\n",
    "    ds_train = preprocess_dataset(ds_train, cfg, n_classes, resize=False, normalize=True, onehot=True)\n",
    "    ds_val = preprocess_dataset(ds_val, cfg, n_classes, resize=False, normalize=True, onehot=True)\n",
    "    ds_test = preprocess_dataset(ds_test, cfg, n_classes, resize=False, normalize=True, onehot=True)\n",
    "\n",
    "    model = tf.keras.models.load_model(f'../results/PI_Explainability/{model_name}_{dataset_name}/run_{run+1}/trained_model.keras')\n",
    "    int_model = tf.keras.Model(inputs=model.inputs, outputs=model.layers[-2].output)\n",
    "    \n",
    "    ##############################################################\n",
    "    #\n",
    "    # Train PSI Model\n",
    "    #\n",
    "    # #############################################################\n",
    "    \n",
    "    print(f'Training PSI model (binning)...')\n",
    "    for n_projs in [50,100,250,500]:\n",
    "        print(f'N_projs: {n_projs}')\n",
    "        \n",
    "        ds_activity = ds_train.batch(cfg['batch_size']).map(lambda x, y: (int_model(x), y)).unbatch()\n",
    "        x, y = zip(*ds_activity)\n",
    "        x = np.array([val.numpy() for val in x])\n",
    "        y = np.array([val.numpy() for val in y])\n",
    "        \n",
    "        psi_data = psi_bin_train(x, y, n_projs, n_bins=50)\n",
    "        np.save(f'{exp_name}/trained_model_{n_projs}_projs.npy', psi_data)\n",
    "    \n",
    "    ##############################################################\n",
    "    #\n",
    "    # Compute PSI for all validation and test samples\n",
    "    #\n",
    "    # #############################################################\n",
    "    \n",
    "        psi_data = np.load(f'{exp_name}/trained_model_{n_projs}_projs.npy', allow_pickle=True).item()\n",
    "        \n",
    "        print(f'Computing PSI for all validation samples and for all classes...')\n",
    "        psi_class = []\n",
    "        for k in range(n_classes):\n",
    "            ds_activity = ds_val.batch(cfg['batch_size']).map(lambda x, y: (int_model(x),tf.one_hot(tf.fill([tf.shape(x)[0]], k), depth=n_classes))).unbatch()\n",
    "            x, y = zip(*ds_activity)\n",
    "            x = np.array([val.numpy() for val in x])\n",
    "            y = np.array([val.numpy() for val in y])\n",
    "            psi, pmi_arr = psi_bin_val(x, y, psi_data, n_projs)\n",
    "            psi_class.append(psi)\n",
    "        np.save(f'{exp_name}/psi_class_{n_projs}_projs_val.npy', np.array(psi_class).T)\n",
    "                \n",
    "        print(f'Computing PSI for all test samples and for all classes...')\n",
    "        psi_class = []\n",
    "        for k in range(n_classes):\n",
    "            ds_activity = ds_test.batch(cfg['batch_size']).map(lambda x, y: (int_model(x),tf.one_hot(tf.fill([tf.shape(x)[0]], k), depth=n_classes))).unbatch()\n",
    "            x, y = zip(*ds_activity)\n",
    "            x = np.array([val.numpy() for val in x])\n",
    "            y = np.array([val.numpy() for val in y])\n",
    "            psi, pmi_arr = psi_bin_val(x, y, psi_data, n_projs)\n",
    "            psi_class.append(psi)\n",
    "        np.save(f'{exp_name}/psi_class_{n_projs}_projs_test.npy', np.array(psi_class).T)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4764b9ed",
   "metadata": {},
   "source": [
    "### Gaussian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "723f6ee2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training PSI model (gaussian)...\n",
      "N_projs: 50\n",
      "Computing PSI for all validation samples and for all classes...\n",
      "Computing PSI for all test samples and for all classes...\n",
      "N_projs: 100\n",
      "Computing PSI for all validation samples and for all classes...\n",
      "Computing PSI for all test samples and for all classes...\n",
      "N_projs: 250\n",
      "Computing PSI for all validation samples and for all classes...\n",
      "Computing PSI for all test samples and for all classes...\n",
      "N_projs: 500\n",
      "Computing PSI for all validation samples and for all classes...\n",
      "Computing PSI for all test samples and for all classes...\n",
      "Training PSI model (gaussian)...\n",
      "N_projs: 50\n",
      "Computing PSI for all validation samples and for all classes...\n",
      "Computing PSI for all test samples and for all classes...\n",
      "N_projs: 100\n",
      "Computing PSI for all validation samples and for all classes...\n",
      "Computing PSI for all test samples and for all classes...\n",
      "N_projs: 250\n",
      "Computing PSI for all validation samples and for all classes...\n",
      "Computing PSI for all test samples and for all classes...\n",
      "N_projs: 500\n",
      "Computing PSI for all validation samples and for all classes...\n",
      "Computing PSI for all test samples and for all classes...\n",
      "Training PSI model (gaussian)...\n",
      "N_projs: 50\n",
      "Computing PSI for all validation samples and for all classes...\n",
      "Computing PSI for all test samples and for all classes...\n",
      "N_projs: 100\n",
      "Computing PSI for all validation samples and for all classes...\n",
      "Computing PSI for all test samples and for all classes...\n",
      "N_projs: 250\n",
      "Computing PSI for all validation samples and for all classes...\n",
      "Computing PSI for all test samples and for all classes...\n",
      "N_projs: 500\n",
      "Computing PSI for all validation samples and for all classes...\n",
      "Computing PSI for all test samples and for all classes...\n",
      "Training PSI model (gaussian)...\n",
      "N_projs: 50\n",
      "Computing PSI for all validation samples and for all classes...\n",
      "Computing PSI for all test samples and for all classes...\n",
      "N_projs: 100\n",
      "Computing PSI for all validation samples and for all classes...\n",
      "Computing PSI for all test samples and for all classes...\n",
      "N_projs: 250\n",
      "Computing PSI for all validation samples and for all classes...\n",
      "Computing PSI for all test samples and for all classes...\n",
      "N_projs: 500\n",
      "Computing PSI for all validation samples and for all classes...\n",
      "Computing PSI for all test samples and for all classes...\n",
      "Training PSI model (gaussian)...\n",
      "N_projs: 50\n",
      "Computing PSI for all validation samples and for all classes...\n",
      "Computing PSI for all test samples and for all classes...\n",
      "N_projs: 100\n",
      "Computing PSI for all validation samples and for all classes...\n",
      "Computing PSI for all test samples and for all classes...\n",
      "N_projs: 250\n",
      "Computing PSI for all validation samples and for all classes...\n",
      "Computing PSI for all test samples and for all classes...\n",
      "N_projs: 500\n",
      "Computing PSI for all validation samples and for all classes...\n",
      "Computing PSI for all test samples and for all classes...\n"
     ]
    }
   ],
   "source": [
    "for run in range(5):\n",
    "    tf.keras.utils.set_random_seed(run+10) # set random seed for Python, NumPy, and TensorFlow\n",
    "    exp_name = f'../results/PI_Explainability/{model_name}_{dataset_name}/run_{run+1}/calibration/psi/gaussian'\n",
    "    if not os.path.exists(exp_name):\n",
    "        print(\"Making directory\", exp_name)\n",
    "        os.makedirs(exp_name)\n",
    "\n",
    "    ds_train, ds_val, ds_test, ds_info = load_dataset(cfg)\n",
    "    n_classes = ds_info.features['label'].num_classes\n",
    "    ds_train = preprocess_dataset(ds_train, cfg, n_classes, resize=False, normalize=True, onehot=True)\n",
    "    ds_val = preprocess_dataset(ds_val, cfg, n_classes, resize=False, normalize=True, onehot=True)\n",
    "    ds_test = preprocess_dataset(ds_test, cfg, n_classes, resize=False, normalize=True, onehot=True)\n",
    "\n",
    "    model = tf.keras.models.load_model(f'../results/PI_Explainability/{model_name}_{dataset_name}/run_{run+1}/trained_model.keras')\n",
    "    int_model = tf.keras.Model(inputs=model.inputs, outputs=model.layers[-2].output)\n",
    "    \n",
    "    ##############################################################\n",
    "    #\n",
    "    # Train PSI Model\n",
    "    #\n",
    "    # #############################################################\n",
    "    \n",
    "    print(f'Training PSI model (gaussian)...')\n",
    "    \n",
    "    for n_projs in [50,100,250,500]:\n",
    "        print(f'N_projs: {n_projs}')\n",
    "        \n",
    "        ds_activity = ds_train.batch(cfg['batch_size']).map(lambda x, y: (int_model(x), y)).unbatch()\n",
    "        x, y = zip(*ds_activity)\n",
    "        x = np.array([val.numpy() for val in x])\n",
    "        y = np.array([val.numpy() for val in y])\n",
    "        \n",
    "        psi_data = psi_gauss_train(x, y, n_projs)\n",
    "        np.save(f'{exp_name}/trained_model_{n_projs}_projs.npy', psi_data)\n",
    "    \n",
    "    ##############################################################\n",
    "    #\n",
    "    # Compute PSI for all validation and test samples\n",
    "    #\n",
    "    # #############################################################\n",
    "    \n",
    "        psi_data = np.load(f'{exp_name}/trained_model_{n_projs}_projs.npy', allow_pickle=True).item()\n",
    "        \n",
    "        print(f'Computing PSI for all validation samples and for all classes...')\n",
    "        psi_class = []\n",
    "        for k in range(n_classes):\n",
    "            ds_activity = ds_val.batch(cfg['batch_size']).map(lambda x, y: (int_model(x),tf.one_hot(tf.fill([tf.shape(x)[0]], k), depth=n_classes))).unbatch()\n",
    "            x, y = zip(*ds_activity)\n",
    "            x = np.array([val.numpy() for val in x])\n",
    "            y = np.array([val.numpy() for val in y])\n",
    "            psi, pmi_arr = psi_gauss_val(x, y, psi_data, n_projs)\n",
    "            psi_class.append(psi)\n",
    "        np.save(f'{exp_name}/psi_class_{n_projs}_projs_val.npy', np.array(psi_class).T)\n",
    "                \n",
    "        print(f'Computing PSI for all test samples and for all classes...')\n",
    "        psi_class = []\n",
    "        for k in range(n_classes):\n",
    "            ds_activity = ds_test.batch(cfg['batch_size']).map(lambda x, y: (int_model(x),tf.one_hot(tf.fill([tf.shape(x)[0]], k), depth=n_classes))).unbatch()\n",
    "            x, y = zip(*ds_activity)\n",
    "            x = np.array([val.numpy() for val in x])\n",
    "            y = np.array([val.numpy() for val in y])\n",
    "            psi, pmi_arr = psi_gauss_val(x, y, psi_data, n_projs)\n",
    "            psi_class.append(psi)\n",
    "        np.save(f'{exp_name}/psi_class_{n_projs}_projs_test.npy', np.array(psi_class).T)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26e2aad3",
   "metadata": {},
   "source": [
    "### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d3c20cb3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-01 19:18:57.737374: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1883] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 78835 MB memory:  -> device: 0, name: NVIDIA A100-SXM4-80GB, pci bus id: 0000:47:00.0, compute capability: 8.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N_projs: 50\n",
      "Computing PSI for all validation samples and for all classes...\n",
      "Computing PSI for all test samples and for all classes...\n",
      "N_projs: 100\n",
      "Computing PSI for all validation samples and for all classes...\n",
      "Computing PSI for all test samples and for all classes...\n",
      "N_projs: 250\n",
      "Computing PSI for all validation samples and for all classes...\n",
      "Computing PSI for all test samples and for all classes...\n",
      "N_projs: 500\n",
      "Computing PSI for all validation samples and for all classes...\n",
      "Computing PSI for all test samples and for all classes...\n",
      "N_projs: 50\n",
      "Computing PSI for all validation samples and for all classes...\n",
      "Computing PSI for all test samples and for all classes...\n",
      "N_projs: 100\n",
      "Computing PSI for all validation samples and for all classes...\n",
      "Computing PSI for all test samples and for all classes...\n",
      "N_projs: 250\n",
      "Computing PSI for all validation samples and for all classes...\n",
      "Computing PSI for all test samples and for all classes...\n",
      "N_projs: 500\n",
      "Computing PSI for all validation samples and for all classes...\n",
      "Computing PSI for all test samples and for all classes...\n",
      "N_projs: 50\n",
      "Computing PSI for all validation samples and for all classes...\n",
      "Computing PSI for all test samples and for all classes...\n",
      "N_projs: 100\n",
      "Computing PSI for all validation samples and for all classes...\n",
      "Computing PSI for all test samples and for all classes...\n",
      "N_projs: 250\n",
      "Computing PSI for all validation samples and for all classes...\n",
      "Computing PSI for all test samples and for all classes...\n",
      "N_projs: 500\n",
      "Computing PSI for all validation samples and for all classes...\n",
      "Computing PSI for all test samples and for all classes...\n",
      "N_projs: 50\n",
      "Computing PSI for all validation samples and for all classes...\n",
      "Computing PSI for all test samples and for all classes...\n",
      "N_projs: 100\n",
      "Computing PSI for all validation samples and for all classes...\n",
      "Computing PSI for all test samples and for all classes...\n",
      "N_projs: 250\n",
      "Computing PSI for all validation samples and for all classes...\n",
      "Computing PSI for all test samples and for all classes...\n",
      "N_projs: 500\n",
      "Computing PSI for all validation samples and for all classes...\n",
      "Computing PSI for all test samples and for all classes...\n",
      "N_projs: 50\n",
      "Computing PSI for all validation samples and for all classes...\n",
      "Computing PSI for all test samples and for all classes...\n",
      "N_projs: 100\n",
      "Computing PSI for all validation samples and for all classes...\n",
      "Computing PSI for all test samples and for all classes...\n",
      "N_projs: 250\n",
      "Computing PSI for all validation samples and for all classes...\n",
      "Computing PSI for all test samples and for all classes...\n",
      "N_projs: 500\n",
      "Computing PSI for all validation samples and for all classes...\n",
      "Computing PSI for all test samples and for all classes...\n"
     ]
    }
   ],
   "source": [
    "for run in range(5):\n",
    "    tf.keras.utils.set_random_seed(run+10) # set random seed for Python, NumPy, and TensorFlow\n",
    "    exp_name = f'../results/PI_Explainability/{model_name}_{dataset_name}/run_{run+1}/calibration/psi/random_forest'\n",
    "    if not os.path.exists(exp_name):\n",
    "        print(\"Making directory\", exp_name)\n",
    "        os.makedirs(exp_name)\n",
    "\n",
    "    ds_train, ds_val, ds_test, ds_info = load_dataset(cfg)\n",
    "    n_classes = ds_info.features['label'].num_classes\n",
    "    ds_train = preprocess_dataset(ds_train, cfg, n_classes, resize=False, normalize=True, onehot=True)\n",
    "    ds_val = preprocess_dataset(ds_val, cfg, n_classes, resize=False, normalize=True, onehot=True)\n",
    "    ds_test = preprocess_dataset(ds_test, cfg, n_classes, resize=False, normalize=True, onehot=True)\n",
    "\n",
    "    model = tf.keras.models.load_model(f'../results/PI_Explainability/{model_name}_{dataset_name}/run_{run+1}/trained_model.keras')\n",
    "    int_model = tf.keras.Model(inputs=model.inputs, outputs=model.layers[-2].output)\n",
    "    \n",
    "    ##############################################################\n",
    "    #\n",
    "    # Train PSI Model\n",
    "    #\n",
    "    # #############################################################\n",
    "    \n",
    "    print(f'Training PSI model (random forest)...')\n",
    "    ds_activity = ds_train.batch(cfg['batch_size']).map(lambda x, y: (int_model(x), y)).unbatch()\n",
    "    x, y = zip(*ds_activity)\n",
    "    x = np.array([val.numpy() for val in x])\n",
    "    y = np.array([val.numpy() for val in y])\n",
    "    \n",
    "    y = np.argmax([y for x,y in ds_activity], axis=1)\n",
    "    n_class_list = []\n",
    "    for k in range(np.max(y)+1):\n",
    "        idx = np.where(y == k)[0]\n",
    "        n_class_list.append(len(idx))\n",
    "    class_prob = np.array(n_class_list)/len(y)\n",
    "    np.save(f'{exp_name}/class_prob.npy', np.array(class_prob))\n",
    "\n",
    "    psi_rf_train(x, y, n_projs=500, save_path=exp_name)\n",
    "    \n",
    "    ##############################################################\n",
    "    #\n",
    "    # Compute PSI for all validation and test samples\n",
    "    #\n",
    "    # #############################################################\n",
    "    \n",
    "    thetas = np.load(f'{exp_name}/all_thetas.npy')\n",
    "    class_prob = np.load(f'{exp_name}/class_prob.npy')\n",
    "    \n",
    "    for n_projs in [50,100,250,500]:\n",
    "        print(f'N_projs: {n_projs}')\n",
    "        \n",
    "        print(f'Computing PSI for all validation samples and for all classes...')\n",
    "        psi_class = []\n",
    "        for k in range(n_classes):\n",
    "            ds_activity = ds_val.batch(cfg['batch_size']).map(lambda x, y: (int_model(x),y)).unbatch()\n",
    "            x = np.array([val.numpy() for val, _ in ds_activity])\n",
    "            psi, pmi_arr = psi_rf_val(x, k, thetas, class_prob, n_projs, save_path=exp_name)\n",
    "            psi_class.append(psi)\n",
    "        np.save(f'{exp_name}/psi_class_{n_projs}_projs_val.npy', np.array(psi_class).T)\n",
    "        \n",
    "        print(f'Computing PSI for all test samples and for all classes...')\n",
    "        psi_class = []\n",
    "        for k in range(n_classes):\n",
    "            ds_activity = ds_test.batch(cfg['batch_size']).map(lambda x, y: (int_model(x),y)).unbatch()\n",
    "            x = np.array([val.numpy() for val, _ in ds_activity])\n",
    "            psi, pmi_arr = psi_rf_val(x, k, thetas, class_prob, n_projs, save_path=exp_name)\n",
    "            psi_class.append(psi)\n",
    "        np.save(f'{exp_name}/psi_class_{n_projs}_projs_test.npy', np.array(psi_class).T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0c33d8c4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run: 1\n",
      "Estimator: binning\n",
      "N_projs: 50\n",
      "Opt. threshold: -1.226, Test filtering error:2.04\n",
      "N_projs: 100\n",
      "Opt. threshold: -1.193, Test filtering error:2.04\n",
      "N_projs: 250\n",
      "Opt. threshold: -1.014, Test filtering error:1.94\n",
      "N_projs: 500\n",
      "Opt. threshold: -1.153, Test filtering error:2.00\n",
      "Estimator: gaussian\n",
      "N_projs: 50\n",
      "Opt. threshold: -2.119, Test filtering error:2.01\n",
      "N_projs: 100\n",
      "Opt. threshold: -1.890, Test filtering error:2.04\n",
      "N_projs: 250\n",
      "Opt. threshold: -2.255, Test filtering error:2.01\n",
      "N_projs: 500\n",
      "Opt. threshold: -1.767, Test filtering error:2.03\n",
      "Estimator: random_forest\n",
      "N_projs: 50\n",
      "Opt. threshold: -0.756, Test filtering error:1.98\n",
      "N_projs: 100\n",
      "Opt. threshold: -0.695, Test filtering error:2.06\n",
      "N_projs: 250\n",
      "Opt. threshold: -0.447, Test filtering error:2.00\n",
      "N_projs: 500\n",
      "Opt. threshold: -0.525, Test filtering error:1.94\n",
      "Run: 2\n",
      "Estimator: binning\n",
      "N_projs: 50\n",
      "Opt. threshold: -1.171, Test filtering error:1.95\n",
      "N_projs: 100\n",
      "Opt. threshold: -2.636, Test filtering error:1.90\n",
      "N_projs: 250\n",
      "Opt. threshold: -2.237, Test filtering error:1.90\n",
      "N_projs: 500\n",
      "Opt. threshold: -2.248, Test filtering error:1.90\n",
      "Estimator: gaussian\n",
      "N_projs: 50\n",
      "Opt. threshold: -1.455, Test filtering error:1.98\n",
      "N_projs: 100\n",
      "Opt. threshold: -4.347, Test filtering error:1.90\n",
      "N_projs: 250\n",
      "Opt. threshold: -3.309, Test filtering error:1.91\n",
      "N_projs: 500\n",
      "Opt. threshold: -3.293, Test filtering error:1.91\n",
      "Estimator: random_forest\n",
      "N_projs: 50\n",
      "Opt. threshold: -0.986, Test filtering error:1.89\n",
      "N_projs: 100\n",
      "Opt. threshold: -1.418, Test filtering error:1.90\n",
      "N_projs: 250\n",
      "Opt. threshold: -0.560, Test filtering error:1.88\n",
      "N_projs: 500\n",
      "Opt. threshold: -0.576, Test filtering error:1.87\n",
      "Run: 3\n",
      "Estimator: binning\n",
      "N_projs: 50\n",
      "Opt. threshold: -2.507, Test filtering error:1.89\n",
      "N_projs: 100\n",
      "Opt. threshold: -1.221, Test filtering error:1.93\n",
      "N_projs: 250\n",
      "Opt. threshold: -2.247, Test filtering error:1.89\n",
      "N_projs: 500\n",
      "Opt. threshold: -1.447, Test filtering error:1.91\n",
      "Estimator: gaussian\n",
      "N_projs: 50\n",
      "Opt. threshold: -4.092, Test filtering error:1.89\n",
      "N_projs: 100\n",
      "Opt. threshold: -2.397, Test filtering error:1.91\n",
      "N_projs: 250\n",
      "Opt. threshold: -3.671, Test filtering error:1.89\n",
      "N_projs: 500\n",
      "Opt. threshold: -2.020, Test filtering error:1.90\n",
      "Estimator: random_forest\n",
      "N_projs: 50\n",
      "Opt. threshold: -0.944, Test filtering error:1.92\n",
      "N_projs: 100\n",
      "Opt. threshold: -1.103, Test filtering error:1.87\n",
      "N_projs: 250\n",
      "Opt. threshold: -0.806, Test filtering error:1.89\n",
      "N_projs: 500\n",
      "Opt. threshold: -0.729, Test filtering error:1.88\n",
      "Run: 4\n",
      "Estimator: binning\n",
      "N_projs: 50\n",
      "Opt. threshold: -1.101, Test filtering error:1.90\n",
      "N_projs: 100\n",
      "Opt. threshold: -1.495, Test filtering error:1.93\n",
      "N_projs: 250\n",
      "Opt. threshold: -1.354, Test filtering error:1.88\n",
      "N_projs: 500\n",
      "Opt. threshold: -1.109, Test filtering error:1.88\n",
      "Estimator: gaussian\n",
      "N_projs: 50\n",
      "Opt. threshold: -1.462, Test filtering error:1.89\n",
      "N_projs: 100\n",
      "Opt. threshold: -2.433, Test filtering error:1.93\n",
      "N_projs: 250\n",
      "Opt. threshold: -2.077, Test filtering error:1.92\n",
      "N_projs: 500\n",
      "Opt. threshold: -2.122, Test filtering error:1.91\n",
      "Estimator: random_forest\n",
      "N_projs: 50\n",
      "Opt. threshold: -1.037, Test filtering error:1.89\n",
      "N_projs: 100\n",
      "Opt. threshold: -0.814, Test filtering error:1.89\n",
      "N_projs: 250\n",
      "Opt. threshold: -0.786, Test filtering error:1.90\n",
      "N_projs: 500\n",
      "Opt. threshold: -0.710, Test filtering error:1.89\n",
      "Run: 5\n",
      "Estimator: binning\n",
      "N_projs: 50\n",
      "Opt. threshold: -1.457, Test filtering error:1.99\n",
      "N_projs: 100\n",
      "Opt. threshold: -2.050, Test filtering error:2.01\n",
      "N_projs: 250\n",
      "Opt. threshold: -2.332, Test filtering error:2.02\n",
      "N_projs: 500\n",
      "Opt. threshold: -2.055, Test filtering error:2.01\n",
      "Estimator: gaussian\n",
      "N_projs: 50\n",
      "Opt. threshold: -3.262, Test filtering error:2.02\n",
      "N_projs: 100\n",
      "Opt. threshold: -3.198, Test filtering error:2.02\n",
      "N_projs: 250\n",
      "Opt. threshold: -3.290, Test filtering error:2.02\n",
      "N_projs: 500\n",
      "Opt. threshold: -3.069, Test filtering error:2.03\n",
      "Estimator: random_forest\n",
      "N_projs: 50\n",
      "Opt. threshold: -0.926, Test filtering error:1.98\n",
      "N_projs: 100\n",
      "Opt. threshold: -0.802, Test filtering error:1.98\n",
      "N_projs: 250\n",
      "Opt. threshold: -0.704, Test filtering error:1.98\n",
      "N_projs: 500\n",
      "Opt. threshold: -0.703, Test filtering error:1.99\n"
     ]
    }
   ],
   "source": [
    "##############################################################\n",
    "#\n",
    "# Compute Filtering Accuracy (without softmax scaling)\n",
    "#\n",
    "# #############################################################\n",
    "\n",
    "estimators_list = ['binning', 'gaussian', 'random_forest']\n",
    "n_projs_list = [50,100,250,500]\n",
    "\n",
    "for run in range(5):\n",
    "    tf.keras.utils.set_random_seed(run+10) # set random seed for Python, NumPy, and TensorFlow\n",
    "    print(f'Run: {run+1}')\n",
    "    for estimator in estimators_list:\n",
    "        print(f'Estimator: {estimator}')\n",
    "        exp_name = f'../results/PI_Explainability/{model_name}_{dataset_name}/run_{run+1}/calibration/psi/{estimator}'\n",
    "\n",
    "        ds_train, ds_val, ds_test, ds_info = load_dataset(cfg)\n",
    "        n_classes = ds_info.features['label'].num_classes\n",
    "        ds_val = preprocess_dataset(ds_val, cfg, n_classes, resize=False, normalize=True, onehot=True)\n",
    "        ds_test = preprocess_dataset(ds_test, cfg, n_classes, resize=False, normalize=True, onehot=True)\n",
    "        model = tf.keras.models.load_model(f'../results/PI_Explainability/{model_name}_{dataset_name}/run_{run+1}/trained_model.keras')\n",
    "        \n",
    "        for n_projs in n_projs_list:\n",
    "            print(f'N_projs: {n_projs}')\n",
    "            true_y = np.argmax([y for x,y in ds_val], axis=1)\n",
    "            pred_y = np.argmax(model.predict(ds_val.batch(cfg['batch_size']), verbose=0), axis=1)\n",
    "            true_label = np.equal(true_y, pred_y).astype(int) # assign 0 if true_y != pred_y, assign 1 if true_y == pred_y\n",
    "            psi_class = np.load(f'{exp_name}/psi_class_{n_projs}_projs_val.npy')\n",
    "            psi = [psi_value[pred_value] for psi_value, pred_value in zip(psi_class, pred_y)]\n",
    "            opt_threshold = compute_opt_threshold(psi, true_label)\n",
    "\n",
    "            true_y = np.argmax([y for x,y in ds_test], axis=1)\n",
    "            pred_y = np.argmax(model.predict(ds_test.batch(cfg['batch_size']), verbose=0), axis=1)\n",
    "            true_label = np.equal(true_y, pred_y).astype(int) # assign 0 if true_y != pred_y, assign 1 if true_y == pred_y\n",
    "            psi_class = np.load(f'{exp_name}/psi_class_{n_projs}_projs_test.npy')\n",
    "            psi = [psi_value[pred_value] for psi_value, pred_value in zip(psi_class, pred_y)]\n",
    "            test_filtering_acc = compute_filtering_acc(psi, true_label, opt_threshold)\n",
    "\n",
    "            np.savez(f'{exp_name}/unscaled_filtering_accuracy_{n_projs}_projs.npz', opt_threshold=opt_threshold, test_filtering_acc=test_filtering_acc)\n",
    "            print(f'Opt. threshold: {opt_threshold:.3f}, Test filtering error:{100-test_filtering_acc:.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fc327d85",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------\n",
      "Estimator: binning, N_projs: 50\n",
      "Average opt. threshold: -1.492, std: 0.521\n",
      "Average test filtering error: 1.95, std: 0.06\n",
      "-----------------------------\n",
      "Estimator: binning, N_projs: 100\n",
      "Average opt. threshold: -1.719, std: 0.552\n",
      "Average test filtering error: 1.96, std: 0.05\n",
      "-----------------------------\n",
      "Estimator: binning, N_projs: 250\n",
      "Average opt. threshold: -1.837, std: 0.545\n",
      "Average test filtering error: 1.93, std: 0.05\n",
      "-----------------------------\n",
      "Estimator: binning, N_projs: 500\n",
      "Average opt. threshold: -1.602, std: 0.467\n",
      "Average test filtering error: 1.94, std: 0.05\n",
      "-----------------------------\n",
      "Estimator: gaussian, N_projs: 50\n",
      "Average opt. threshold: -2.478, std: 1.042\n",
      "Average test filtering error: 1.96, std: 0.06\n",
      "-----------------------------\n",
      "Estimator: gaussian, N_projs: 100\n",
      "Average opt. threshold: -2.853, std: 0.856\n",
      "Average test filtering error: 1.96, std: 0.06\n",
      "-----------------------------\n",
      "Estimator: gaussian, N_projs: 250\n",
      "Average opt. threshold: -2.920, std: 0.633\n",
      "Average test filtering error: 1.95, std: 0.05\n",
      "-----------------------------\n",
      "Estimator: gaussian, N_projs: 500\n",
      "Average opt. threshold: -2.454, std: 0.609\n",
      "Average test filtering error: 1.96, std: 0.06\n",
      "-----------------------------\n",
      "Estimator: random_forest, N_projs: 50\n",
      "Average opt. threshold: -0.930, std: 0.095\n",
      "Average test filtering error: 1.93, std: 0.04\n",
      "-----------------------------\n",
      "Estimator: random_forest, N_projs: 100\n",
      "Average opt. threshold: -0.966, std: 0.263\n",
      "Average test filtering error: 1.94, std: 0.07\n",
      "-----------------------------\n",
      "Estimator: random_forest, N_projs: 250\n",
      "Average opt. threshold: -0.660, std: 0.138\n",
      "Average test filtering error: 1.93, std: 0.05\n",
      "-----------------------------\n",
      "Estimator: random_forest, N_projs: 500\n",
      "Average opt. threshold: -0.648, std: 0.082\n",
      "Average test filtering error: 1.91, std: 0.04\n"
     ]
    }
   ],
   "source": [
    "estimators_list = ['binning', 'gaussian', 'random_forest']\n",
    "n_projs_list = [50,100,250,500]\n",
    "\n",
    "for estimator in estimators_list:\n",
    "    for n_projs in n_projs_list:\n",
    "        threshold = []\n",
    "        filtering_acc = []\n",
    "        for run in range(5):\n",
    "            tf.keras.utils.set_random_seed(run+10) # set random seed for Python, NumPy, and TensorFlow\n",
    "            exp_name = f'../results/PI_Explainability/{model_name}_{dataset_name}/run_{run+1}/calibration/psi/{estimator}'\n",
    "            f = np.load(f'{exp_name}/unscaled_filtering_accuracy_{n_projs}_projs.npz')\n",
    "            opt_threshold = f['opt_threshold']\n",
    "            test_filtering_acc = f['test_filtering_acc']\n",
    "            threshold.append(opt_threshold)\n",
    "            filtering_acc.append(test_filtering_acc)\n",
    "\n",
    "        print('-----------------------------')\n",
    "        print(f'Estimator: {estimator}, N_projs: {n_projs}')\n",
    "        print(f'Average opt. threshold: {(np.mean(threshold)):.3f}, std: {(np.std(threshold)):.3f}')\n",
    "        print(f'Average test filtering error: {(100-np.mean(filtering_acc)):.2f}, std: {(np.std(filtering_acc)):.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "75559f8a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run: 1\n",
      "Estimator: binning\n",
      "N_projs: 50\n",
      "Opt. threshold: 0.144, Test filtering error:1.92\n",
      "N_projs: 100\n",
      "Opt. threshold: 0.139, Test filtering error:2.03\n",
      "N_projs: 250\n",
      "Opt. threshold: 0.184, Test filtering error:1.88\n",
      "N_projs: 500\n",
      "Opt. threshold: 0.176, Test filtering error:1.91\n",
      "Estimator: gaussian\n",
      "N_projs: 50\n",
      "Opt. threshold: 0.199, Test filtering error:1.89\n",
      "N_projs: 100\n",
      "Opt. threshold: 0.128, Test filtering error:2.04\n",
      "N_projs: 250\n",
      "Opt. threshold: 0.198, Test filtering error:1.87\n",
      "N_projs: 500\n",
      "Opt. threshold: 0.213, Test filtering error:1.91\n",
      "Estimator: random_forest\n",
      "N_projs: 50\n",
      "Opt. threshold: 0.154, Test filtering error:1.97\n",
      "N_projs: 100\n",
      "Opt. threshold: 0.169, Test filtering error:2.01\n",
      "N_projs: 250\n",
      "Opt. threshold: 0.174, Test filtering error:1.98\n",
      "N_projs: 500\n",
      "Opt. threshold: 0.184, Test filtering error:1.89\n",
      "Run: 2\n",
      "Estimator: binning\n",
      "N_projs: 50\n",
      "Opt. threshold: 0.115, Test filtering error:1.93\n",
      "N_projs: 100\n",
      "Opt. threshold: 0.031, Test filtering error:1.92\n",
      "N_projs: 250\n",
      "Opt. threshold: 0.045, Test filtering error:1.90\n",
      "N_projs: 500\n",
      "Opt. threshold: 0.155, Test filtering error:1.92\n",
      "Estimator: gaussian\n",
      "N_projs: 50\n",
      "Opt. threshold: 0.122, Test filtering error:1.92\n",
      "N_projs: 100\n",
      "Opt. threshold: 0.009, Test filtering error:1.90\n",
      "N_projs: 250\n",
      "Opt. threshold: 0.174, Test filtering error:1.92\n",
      "N_projs: 500\n",
      "Opt. threshold: 0.186, Test filtering error:1.90\n",
      "Estimator: random_forest\n",
      "N_projs: 50\n",
      "Opt. threshold: 0.097, Test filtering error:1.87\n",
      "N_projs: 100\n",
      "Opt. threshold: 0.100, Test filtering error:1.88\n",
      "N_projs: 250\n",
      "Opt. threshold: 0.169, Test filtering error:1.87\n",
      "N_projs: 500\n",
      "Opt. threshold: 0.166, Test filtering error:1.87\n",
      "Run: 3\n",
      "Estimator: binning\n",
      "N_projs: 50\n",
      "Opt. threshold: 0.162, Test filtering error:1.93\n",
      "N_projs: 100\n",
      "Opt. threshold: 0.138, Test filtering error:1.91\n",
      "N_projs: 250\n",
      "Opt. threshold: 0.132, Test filtering error:1.92\n",
      "N_projs: 500\n",
      "Opt. threshold: 0.162, Test filtering error:1.90\n",
      "Estimator: gaussian\n",
      "N_projs: 50\n",
      "Opt. threshold: 0.118, Test filtering error:1.93\n",
      "N_projs: 100\n",
      "Opt. threshold: 0.172, Test filtering error:1.92\n",
      "N_projs: 250\n",
      "Opt. threshold: 0.233, Test filtering error:1.86\n",
      "N_projs: 500\n",
      "Opt. threshold: 0.179, Test filtering error:1.90\n",
      "Estimator: random_forest\n",
      "N_projs: 50\n",
      "Opt. threshold: 0.106, Test filtering error:1.88\n",
      "N_projs: 100\n",
      "Opt. threshold: 0.087, Test filtering error:1.87\n",
      "N_projs: 250\n",
      "Opt. threshold: 0.164, Test filtering error:1.90\n",
      "N_projs: 500\n",
      "Opt. threshold: 0.181, Test filtering error:1.89\n",
      "Run: 4\n",
      "Estimator: binning\n",
      "N_projs: 50\n",
      "Opt. threshold: 0.163, Test filtering error:1.95\n",
      "N_projs: 100\n",
      "Opt. threshold: 0.134, Test filtering error:1.92\n",
      "N_projs: 250\n",
      "Opt. threshold: 0.190, Test filtering error:1.90\n",
      "N_projs: 500\n",
      "Opt. threshold: 0.200, Test filtering error:1.87\n",
      "Estimator: gaussian\n",
      "N_projs: 50\n",
      "Opt. threshold: 0.210, Test filtering error:1.88\n",
      "N_projs: 100\n",
      "Opt. threshold: 0.254, Test filtering error:1.82\n",
      "N_projs: 250\n",
      "Opt. threshold: 0.241, Test filtering error:1.82\n",
      "N_projs: 500\n",
      "Opt. threshold: 0.281, Test filtering error:1.90\n",
      "Estimator: random_forest\n",
      "N_projs: 50\n",
      "Opt. threshold: 0.125, Test filtering error:1.92\n",
      "N_projs: 100\n",
      "Opt. threshold: 0.182, Test filtering error:1.84\n",
      "N_projs: 250\n",
      "Opt. threshold: 0.210, Test filtering error:1.89\n",
      "N_projs: 500\n",
      "Opt. threshold: 0.201, Test filtering error:1.90\n",
      "Run: 5\n",
      "Estimator: binning\n",
      "N_projs: 50\n",
      "Opt. threshold: 0.085, Test filtering error:2.00\n",
      "N_projs: 100\n",
      "Opt. threshold: 0.056, Test filtering error:2.02\n",
      "N_projs: 250\n",
      "Opt. threshold: 0.046, Test filtering error:2.02\n",
      "N_projs: 500\n",
      "Opt. threshold: 0.062, Test filtering error:2.01\n",
      "Estimator: gaussian\n",
      "N_projs: 50\n",
      "Opt. threshold: 0.124, Test filtering error:2.02\n",
      "N_projs: 100\n",
      "Opt. threshold: 0.033, Test filtering error:2.02\n",
      "N_projs: 250\n",
      "Opt. threshold: 0.032, Test filtering error:2.02\n",
      "N_projs: 500\n",
      "Opt. threshold: 0.080, Test filtering error:2.03\n",
      "Estimator: random_forest\n",
      "N_projs: 50\n",
      "Opt. threshold: 0.093, Test filtering error:1.98\n",
      "N_projs: 100\n",
      "Opt. threshold: 0.111, Test filtering error:1.96\n",
      "N_projs: 250\n",
      "Opt. threshold: 0.121, Test filtering error:1.96\n",
      "N_projs: 500\n",
      "Opt. threshold: 0.198, Test filtering error:1.93\n"
     ]
    }
   ],
   "source": [
    "##############################################################\n",
    "#\n",
    "# Compute Filtering Accuracy (with softmax scaling)\n",
    "#\n",
    "# #############################################################\n",
    "\n",
    "estimators_list = ['binning', 'gaussian', 'random_forest']\n",
    "n_projs_list = [50,100,250,500]\n",
    "\n",
    "for run in range(5):\n",
    "    tf.keras.utils.set_random_seed(run+10) # set random seed for Python, NumPy, and TensorFlow\n",
    "    print(f'Run: {run+1}')\n",
    "    for estimator in estimators_list:\n",
    "        print(f'Estimator: {estimator}')\n",
    "        exp_name = f'../results/PI_Explainability/{model_name}_{dataset_name}/run_{run+1}/calibration/psi/{estimator}'\n",
    "\n",
    "        ds_train, ds_val, ds_test, ds_info = load_dataset(cfg)\n",
    "        n_classes = ds_info.features['label'].num_classes\n",
    "        ds_val = preprocess_dataset(ds_val, cfg, n_classes, resize=False, normalize=True, onehot=True)\n",
    "        ds_test = preprocess_dataset(ds_test, cfg, n_classes, resize=False, normalize=True, onehot=True)\n",
    "        model = tf.keras.models.load_model(f'../results/PI_Explainability/{model_name}_{dataset_name}/run_{run+1}/trained_model.keras')\n",
    "        \n",
    "        for n_projs in n_projs_list:\n",
    "            print(f'N_projs: {n_projs}')\n",
    "            true_y = np.argmax([y for x,y in ds_val], axis=1)\n",
    "            pred_y = np.argmax(model.predict(ds_val.batch(cfg['batch_size']), verbose=0), axis=1)\n",
    "            true_label = np.equal(true_y, pred_y).astype(int) # assign 0 if true_y != pred_y, assign 1 if true_y == pred_y\n",
    "            psi_class = np.load(f'{exp_name}/psi_class_{n_projs}_projs_val.npy')\n",
    "            psi_class = np.array([softmax(x) for x in psi_class])\n",
    "            psi = [psi_value[pred_value] for psi_value, pred_value in zip(psi_class, pred_y)]\n",
    "            opt_threshold = compute_opt_threshold(psi, true_label)\n",
    "\n",
    "            true_y = np.argmax([y for x,y in ds_test], axis=1)\n",
    "            pred_y = np.argmax(model.predict(ds_test.batch(cfg['batch_size']), verbose=0), axis=1)\n",
    "            true_label = np.equal(true_y, pred_y).astype(int) # assign 0 if true_y != pred_y, assign 1 if true_y == pred_y\n",
    "            psi_class = np.load(f'{exp_name}/psi_class_{n_projs}_projs_test.npy')\n",
    "            psi_class = np.array([softmax(x) for x in psi_class])\n",
    "            psi = [psi_value[pred_value] for psi_value, pred_value in zip(psi_class, pred_y)]\n",
    "            test_filtering_acc = compute_filtering_acc(psi, true_label, opt_threshold)\n",
    "\n",
    "            np.savez(f'{exp_name}/scaled_filtering_accuracy_{n_projs}_projs.npz', opt_threshold=opt_threshold, test_filtering_acc=test_filtering_acc)\n",
    "            print(f'Opt. threshold: {opt_threshold:.3f}, Test filtering error:{100-test_filtering_acc:.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "69e6ec29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------\n",
      "Estimator: binning, N_projs: 50\n",
      "Average opt. threshold: 0.134, std: 0.030\n",
      "Average test filtering error: 1.95, std: 0.03\n",
      "-----------------------------\n",
      "Estimator: binning, N_projs: 100\n",
      "Average opt. threshold: 0.100, std: 0.046\n",
      "Average test filtering error: 1.96, std: 0.05\n",
      "-----------------------------\n",
      "Estimator: binning, N_projs: 250\n",
      "Average opt. threshold: 0.119, std: 0.064\n",
      "Average test filtering error: 1.92, std: 0.05\n",
      "-----------------------------\n",
      "Estimator: binning, N_projs: 500\n",
      "Average opt. threshold: 0.151, std: 0.047\n",
      "Average test filtering error: 1.92, std: 0.05\n",
      "-----------------------------\n",
      "Estimator: gaussian, N_projs: 50\n",
      "Average opt. threshold: 0.154, std: 0.041\n",
      "Average test filtering error: 1.93, std: 0.05\n",
      "-----------------------------\n",
      "Estimator: gaussian, N_projs: 100\n",
      "Average opt. threshold: 0.119, std: 0.090\n",
      "Average test filtering error: 1.94, std: 0.08\n",
      "-----------------------------\n",
      "Estimator: gaussian, N_projs: 250\n",
      "Average opt. threshold: 0.176, std: 0.076\n",
      "Average test filtering error: 1.90, std: 0.07\n",
      "-----------------------------\n",
      "Estimator: gaussian, N_projs: 500\n",
      "Average opt. threshold: 0.188, std: 0.065\n",
      "Average test filtering error: 1.93, std: 0.05\n",
      "-----------------------------\n",
      "Estimator: random_forest, N_projs: 50\n",
      "Average opt. threshold: 0.115, std: 0.022\n",
      "Average test filtering error: 1.92, std: 0.04\n",
      "-----------------------------\n",
      "Estimator: random_forest, N_projs: 100\n",
      "Average opt. threshold: 0.130, std: 0.038\n",
      "Average test filtering error: 1.91, std: 0.06\n",
      "-----------------------------\n",
      "Estimator: random_forest, N_projs: 250\n",
      "Average opt. threshold: 0.167, std: 0.028\n",
      "Average test filtering error: 1.92, std: 0.04\n",
      "-----------------------------\n",
      "Estimator: random_forest, N_projs: 500\n",
      "Average opt. threshold: 0.186, std: 0.013\n",
      "Average test filtering error: 1.90, std: 0.02\n"
     ]
    }
   ],
   "source": [
    "estimators_list = ['binning', 'gaussian', 'random_forest']\n",
    "n_projs_list = [50,100,250,500]\n",
    "\n",
    "for estimator in estimators_list:\n",
    "    for n_projs in n_projs_list:\n",
    "        threshold = []\n",
    "        filtering_acc = []\n",
    "        for run in range(5):\n",
    "            tf.keras.utils.set_random_seed(run+10) # set random seed for Python, NumPy, and TensorFlow\n",
    "            exp_name = f'../results/PI_Explainability/{model_name}_{dataset_name}/run_{run+1}/calibration/psi/{estimator}'\n",
    "            f = np.load(f'{exp_name}/scaled_filtering_accuracy_{n_projs}_projs.npz')\n",
    "            opt_threshold = f['opt_threshold']\n",
    "            test_filtering_acc = f['test_filtering_acc']\n",
    "            threshold.append(opt_threshold)\n",
    "            filtering_acc.append(test_filtering_acc)\n",
    "\n",
    "        print('-----------------------------')\n",
    "        print(f'Estimator: {estimator}, N_projs: {n_projs}')\n",
    "        print(f'Average opt. threshold: {(np.mean(threshold)):.3f}, std: {(np.std(threshold)):.3f}')\n",
    "        print(f'Average test filtering error: {(100-np.mean(filtering_acc)):.2f}, std: {(np.std(filtering_acc)):.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "838ae640",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run: 1\n",
      "Estimator: binning\n",
      "N_projs: 50\n",
      "ECE: 30.38\n",
      "N_projs: 100\n",
      "ECE: 30.08\n",
      "N_projs: 250\n",
      "ECE: 29.06\n",
      "N_projs: 500\n",
      "ECE: 27.52\n",
      "Estimator: gaussian\n",
      "N_projs: 50\n",
      "ECE: 16.12\n",
      "N_projs: 100\n",
      "ECE: 16.30\n",
      "N_projs: 250\n",
      "ECE: 14.80\n",
      "N_projs: 500\n",
      "ECE: 13.80\n",
      "Estimator: random_forest\n",
      "N_projs: 50\n",
      "ECE: 32.34\n",
      "N_projs: 100\n",
      "ECE: 31.27\n",
      "N_projs: 250\n",
      "ECE: 31.79\n",
      "N_projs: 500\n",
      "ECE: 29.78\n",
      "Run: 2\n",
      "Estimator: binning\n",
      "N_projs: 50\n",
      "ECE: 32.10\n",
      "N_projs: 100\n",
      "ECE: 28.92\n",
      "N_projs: 250\n",
      "ECE: 28.84\n",
      "N_projs: 500\n",
      "ECE: 29.60\n",
      "Estimator: gaussian\n",
      "N_projs: 50\n",
      "ECE: 17.63\n",
      "N_projs: 100\n",
      "ECE: 15.26\n",
      "N_projs: 250\n",
      "ECE: 14.90\n",
      "N_projs: 500\n",
      "ECE: 15.40\n",
      "Estimator: random_forest\n",
      "N_projs: 50\n",
      "ECE: 35.23\n",
      "N_projs: 100\n",
      "ECE: 33.07\n",
      "N_projs: 250\n",
      "ECE: 31.40\n",
      "N_projs: 500\n",
      "ECE: 31.00\n",
      "Run: 3\n",
      "Estimator: binning\n",
      "N_projs: 50\n",
      "ECE: 29.26\n",
      "N_projs: 100\n",
      "ECE: 27.06\n",
      "N_projs: 250\n",
      "ECE: 27.04\n",
      "N_projs: 500\n",
      "ECE: 26.88\n",
      "Estimator: gaussian\n",
      "N_projs: 50\n",
      "ECE: 15.46\n",
      "N_projs: 100\n",
      "ECE: 13.78\n",
      "N_projs: 250\n",
      "ECE: 13.15\n",
      "N_projs: 500\n",
      "ECE: 13.28\n",
      "Estimator: random_forest\n",
      "N_projs: 50\n",
      "ECE: 32.58\n",
      "N_projs: 100\n",
      "ECE: 29.78\n",
      "N_projs: 250\n",
      "ECE: 29.16\n",
      "N_projs: 500\n",
      "ECE: 30.00\n",
      "Run: 4\n",
      "Estimator: binning\n",
      "N_projs: 50\n",
      "ECE: 29.10\n",
      "N_projs: 100\n",
      "ECE: 27.71\n",
      "N_projs: 250\n",
      "ECE: 27.52\n",
      "N_projs: 500\n",
      "ECE: 26.35\n",
      "Estimator: gaussian\n",
      "N_projs: 50\n",
      "ECE: 14.71\n",
      "N_projs: 100\n",
      "ECE: 13.25\n",
      "N_projs: 250\n",
      "ECE: 13.00\n",
      "N_projs: 500\n",
      "ECE: 12.10\n",
      "Estimator: random_forest\n",
      "N_projs: 50\n",
      "ECE: 32.09\n",
      "N_projs: 100\n",
      "ECE: 29.84\n",
      "N_projs: 250\n",
      "ECE: 30.33\n",
      "N_projs: 500\n",
      "ECE: 29.40\n",
      "Run: 5\n",
      "Estimator: binning\n",
      "N_projs: 50\n",
      "ECE: 30.62\n",
      "N_projs: 100\n",
      "ECE: 27.26\n",
      "N_projs: 250\n",
      "ECE: 26.65\n",
      "N_projs: 500\n",
      "ECE: 27.02\n",
      "Estimator: gaussian\n",
      "N_projs: 50\n",
      "ECE: 16.71\n",
      "N_projs: 100\n",
      "ECE: 13.17\n",
      "N_projs: 250\n",
      "ECE: 12.99\n",
      "N_projs: 500\n",
      "ECE: 13.38\n",
      "Estimator: random_forest\n",
      "N_projs: 50\n",
      "ECE: 34.12\n",
      "N_projs: 100\n",
      "ECE: 31.75\n",
      "N_projs: 250\n",
      "ECE: 29.70\n",
      "N_projs: 500\n",
      "ECE: 29.06\n"
     ]
    }
   ],
   "source": [
    "##############################################################\n",
    "#\n",
    "# Compute ECE (with softmax scaling)\n",
    "#\n",
    "# #############################################################\n",
    "\n",
    "estimators_list = ['binning', 'gaussian', 'random_forest']\n",
    "n_projs_list = [50,100,250,500]\n",
    "\n",
    "for run in range(5):\n",
    "    tf.keras.utils.set_random_seed(run+10) # set random seed for Python, NumPy, and TensorFlow\n",
    "    print(f'Run: {run+1}')\n",
    "    for estimator in estimators_list:\n",
    "        print(f'Estimator: {estimator}')\n",
    "        exp_name = f'../results/PI_Explainability/{model_name}_{dataset_name}/run_{run+1}/calibration/psi/{estimator}'\n",
    "\n",
    "        ds_train, ds_val, ds_test, ds_info = load_dataset(cfg)\n",
    "        n_classes = ds_info.features['label'].num_classes\n",
    "        ds_test = preprocess_dataset(ds_test, cfg, n_classes, resize=False, normalize=True, onehot=True)\n",
    "        model = tf.keras.models.load_model(f'../results/PI_Explainability/{model_name}_{dataset_name}/run_{run+1}/trained_model.keras')\n",
    "        \n",
    "        for n_projs in n_projs_list:\n",
    "            print(f'N_projs: {n_projs}')\n",
    "            true_y = np.argmax([y for x,y in ds_test], axis=1)\n",
    "            pred_y = np.argmax(model.predict(ds_test.batch(cfg['batch_size']), verbose=0), axis=1)\n",
    "            \n",
    "            psi_class = np.load(f'{exp_name}/psi_class_{n_projs}_projs_test.npy')\n",
    "            psi_class = np.array([softmax(x) for x in psi_class])\n",
    "            psi = np.array([psi_value[pred_value] for psi_value, pred_value in zip(psi_class, pred_y)])\n",
    "            ece = compute_ece(psi, true_y, pred_y, n_bins=10)\n",
    "\n",
    "            np.save(f'{exp_name}/ece_test_{n_projs}_projs.npy', ece)\n",
    "            print(f'ECE: {ece:.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3ddbea4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------\n",
      "Estimator: binning, N_projs: 50\n",
      "Average ECE: 30.29, std: 1.08\n",
      "-----------------------------\n",
      "Estimator: binning, N_projs: 100\n",
      "Average ECE: 28.21, std: 1.14\n",
      "-----------------------------\n",
      "Estimator: binning, N_projs: 250\n",
      "Average ECE: 27.82, std: 0.96\n",
      "-----------------------------\n",
      "Estimator: binning, N_projs: 500\n",
      "Average ECE: 27.47, std: 1.13\n",
      "-----------------------------\n",
      "Estimator: gaussian, N_projs: 50\n",
      "Average ECE: 16.13, std: 1.00\n",
      "-----------------------------\n",
      "Estimator: gaussian, N_projs: 100\n",
      "Average ECE: 14.35, std: 1.23\n",
      "-----------------------------\n",
      "Estimator: gaussian, N_projs: 250\n",
      "Average ECE: 13.77, std: 0.89\n",
      "-----------------------------\n",
      "Estimator: gaussian, N_projs: 500\n",
      "Average ECE: 13.59, std: 1.07\n",
      "-----------------------------\n",
      "Estimator: random_forest, N_projs: 50\n",
      "Average ECE: 33.27, std: 1.21\n",
      "-----------------------------\n",
      "Estimator: random_forest, N_projs: 100\n",
      "Average ECE: 31.14, std: 1.24\n",
      "-----------------------------\n",
      "Estimator: random_forest, N_projs: 250\n",
      "Average ECE: 30.47, std: 0.99\n",
      "-----------------------------\n",
      "Estimator: random_forest, N_projs: 500\n",
      "Average ECE: 29.85, std: 0.66\n"
     ]
    }
   ],
   "source": [
    "estimators_list = ['binning', 'gaussian', 'random_forest']\n",
    "n_projs_list = [50,100,250,500]\n",
    "\n",
    "for estimator in estimators_list:\n",
    "    for n_projs in n_projs_list:\n",
    "        ece_list = []\n",
    "        for run in range(5):\n",
    "            tf.keras.utils.set_random_seed(run+10) # set random seed for Python, NumPy, and TensorFlow\n",
    "            exp_name = f'../results/PI_Explainability/{model_name}_{dataset_name}/run_{run+1}/calibration/psi/{estimator}'\n",
    "            f = np.load(f'{exp_name}/scaled_filtering_accuracy_{n_projs}_projs.npz')\n",
    "            ece = np.load(f'{exp_name}/ece_test_{n_projs}_projs.npy')\n",
    "            ece_list.append(ece)\n",
    "\n",
    "        print('-----------------------------')\n",
    "        print(f'Estimator: {estimator}, N_projs: {n_projs}')\n",
    "        print(f'Average ECE: {(np.mean(ece_list)):.2f}, std: {(np.std(ece_list)):.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7bbbc563",
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg = { 'dataset' : 'stl10',\n",
    "        'model' : 'pretrained_inception',\n",
    "        'batch_size' : 512,\n",
    "        'optimizer' : 'Adam',\n",
    "        'learning_rate' : 0.001,\n",
    "        'max_epoch' : 300,\n",
    "        'patience' : 10,}    \n",
    "\n",
    "model_name = cfg['model']\n",
    "dataset_name = cfg['dataset']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6803f98",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-21 23:00:41.393121: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1883] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 1136 MB memory:  -> device: 0, name: NVIDIA A100-SXM4-80GB, pci bus id: 0000:47:00.0, compute capability: 8.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training PSI model (gaussian)...\n",
      "Computing PSI for all validation samples and for all classes...\n",
      "Computing PSI for all test samples and for all classes...\n",
      "Training PSI model (gaussian)...\n",
      "Computing PSI for all validation samples and for all classes...\n",
      "Computing PSI for all test samples and for all classes...\n",
      "Training PSI model (gaussian)...\n",
      "Computing PSI for all validation samples and for all classes...\n",
      "Computing PSI for all test samples and for all classes...\n"
     ]
    }
   ],
   "source": [
    "for run in range(5):\n",
    "    tf.keras.utils.set_random_seed(run+10) # set random seed for Python, NumPy, and TensorFlow\n",
    "    exp_name = f'../results/PI_Explainability/{model_name}_{dataset_name}/run_{run+1}/calibration/psi/gaussian'\n",
    "    if not os.path.exists(exp_name):\n",
    "        print(\"Making directory\", exp_name)\n",
    "        os.makedirs(exp_name)\n",
    "\n",
    "    ds_train, ds_val, ds_test, ds_info = load_dataset(cfg, shuffle=False)\n",
    "    n_classes = ds_info.features['label'].num_classes\n",
    "    ds_train = preprocess_dataset(ds_train, cfg, n_classes, resize=True, normalize=True, onehot=True)\n",
    "    ds_val = preprocess_dataset(ds_val, cfg, n_classes, resize=True, normalize=True, onehot=True)\n",
    "    ds_test = preprocess_dataset(ds_test, cfg, n_classes, resize=True, normalize=True, onehot=True)\n",
    "\n",
    "    model = tf.keras.models.load_model(f'../results/PI_Explainability/{model_name}_{dataset_name}/run_{run+1}/trained_model.keras')\n",
    "    int_model = tf.keras.Model(inputs=model.inputs, outputs=model.layers[-2].output)\n",
    "    \n",
    "    ##############################################################\n",
    "    #\n",
    "    # Train PSI Model\n",
    "    #\n",
    "    # #############################################################\n",
    "    \n",
    "    print(f'Training PSI model (gaussian)...')\n",
    "        \n",
    "    ds_activity = ds_train.batch(cfg['batch_size']).map(lambda x, y: (int_model(x), y)).unbatch()\n",
    "    x, y = zip(*ds_activity)\n",
    "    x = np.array([val.numpy() for val in x])\n",
    "    y = np.array([val.numpy() for val in y])\n",
    "\n",
    "    psi_data = psi_gauss_train(x, y, n_projs=250)\n",
    "    np.save(f'{exp_name}/trained_model.npy', psi_data)\n",
    "    \n",
    "    ##############################################################\n",
    "    #\n",
    "    # Compute PSI for all validation and test samples\n",
    "    #\n",
    "    # #############################################################\n",
    "    \n",
    "    psi_data = np.load(f'{exp_name}/trained_model.npy', allow_pickle=True).item()\n",
    "\n",
    "    print(f'Computing PSI for all validation samples and for all classes...')\n",
    "    psi_class = []\n",
    "    for k in range(n_classes):\n",
    "        ds_activity = ds_val.batch(cfg['batch_size']).map(lambda x, y: (int_model(x),tf.one_hot(tf.fill([tf.shape(x)[0]], k), depth=n_classes))).unbatch()\n",
    "        x, y = zip(*ds_activity)\n",
    "        x = np.array([val.numpy() for val in x])\n",
    "        y = np.array([val.numpy() for val in y])\n",
    "        psi, pmi_arr = psi_gauss_val(x, y, psi_data, n_projs=250)\n",
    "        psi_class.append(psi)\n",
    "    np.save(f'{exp_name}/psi_class_val.npy', np.array(psi_class).T)\n",
    "\n",
    "    print(f'Computing PSI for all test samples and for all classes...')\n",
    "    psi_class = []\n",
    "    for k in range(n_classes):\n",
    "        ds_activity = ds_test.batch(cfg['batch_size']).map(lambda x, y: (int_model(x),tf.one_hot(tf.fill([tf.shape(x)[0]], k), depth=n_classes))).unbatch()\n",
    "        x, y = zip(*ds_activity)\n",
    "        x = np.array([val.numpy() for val in x])\n",
    "        y = np.array([val.numpy() for val in y])\n",
    "        psi, pmi_arr = psi_gauss_val(x, y, psi_data, n_projs=250)\n",
    "        psi_class.append(psi)\n",
    "    np.save(f'{exp_name}/psi_class_test.npy', np.array(psi_class).T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1ac5c1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Making directory ../results/PI_Explainability/pretrained_inception_stl10/run_1/calibration/psi/random_forest\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-06 17:10:36.737016: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1883] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 78835 MB memory:  -> device: 0, name: NVIDIA A100-SXM4-80GB, pci bus id: 0000:47:00.0, compute capability: 8.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training PSI model (random forest)...\n",
      "Making directory ../results/PI_Explainability/pretrained_inception_stl10/run_1/calibration/psi/random_forest/psi_models\n",
      "Computing PSI for all validation samples and for all classes...\n",
      "Computing PSI for all test samples and for all classes...\n",
      "Making directory ../results/PI_Explainability/pretrained_inception_stl10/run_2/calibration/psi/random_forest\n",
      "Training PSI model (random forest)...\n",
      "Making directory ../results/PI_Explainability/pretrained_inception_stl10/run_2/calibration/psi/random_forest/psi_models\n",
      "Computing PSI for all validation samples and for all classes...\n",
      "Computing PSI for all test samples and for all classes...\n",
      "Making directory ../results/PI_Explainability/pretrained_inception_stl10/run_3/calibration/psi/random_forest\n",
      "Training PSI model (random forest)...\n",
      "Making directory ../results/PI_Explainability/pretrained_inception_stl10/run_3/calibration/psi/random_forest/psi_models\n",
      "Computing PSI for all validation samples and for all classes...\n",
      "Computing PSI for all test samples and for all classes...\n"
     ]
    }
   ],
   "source": [
    "n_projs = 250\n",
    "\n",
    "for run in range(2,5):\n",
    "    tf.keras.utils.set_random_seed(run+10) # set random seed for Python, NumPy, and TensorFlow\n",
    "    exp_name = f'../results/PI_Explainability/{model_name}_{dataset_name}/run_{run+1}/calibration/psi/random_forest'\n",
    "    if not os.path.exists(exp_name):\n",
    "        print(\"Making directory\", exp_name)\n",
    "        os.makedirs(exp_name)\n",
    "\n",
    "    ds_train, ds_val, ds_test, ds_info = load_dataset(cfg, shuffle=False)\n",
    "    n_classes = ds_info.features['label'].num_classes\n",
    "    ds_train = preprocess_dataset(ds_train, cfg, n_classes, resize=True, normalize=True, onehot=True)\n",
    "    ds_val = preprocess_dataset(ds_val, cfg, n_classes, resize=True, normalize=True, onehot=True)\n",
    "    ds_test = preprocess_dataset(ds_test, cfg, n_classes, resize=True, normalize=True, onehot=True)\n",
    "\n",
    "    model = tf.keras.models.load_model(f'../results/PI_Explainability/{model_name}_{dataset_name}/run_{run+1}/trained_model.keras')\n",
    "    int_model = tf.keras.Model(inputs=model.inputs, outputs=model.layers[-2].output)\n",
    "    \n",
    "    ##############################################################\n",
    "    #\n",
    "    # Train PSI Model\n",
    "    #\n",
    "    # #############################################################\n",
    "    \n",
    "    print(f'Training PSI model (random forest)...')\n",
    "        \n",
    "    ds_activity = ds_train.batch(cfg['batch_size']).map(lambda x, y: (int_model(x), y)).unbatch()\n",
    "    x, y = zip(*ds_activity)\n",
    "    x = np.array([val.numpy() for val in x])\n",
    "    y = np.array([val.numpy() for val in y])\n",
    "    \n",
    "    y = np.argmax([y for x,y in ds_activity], axis=1)\n",
    "    n_class_list = []\n",
    "    for k in range(np.max(y)+1):\n",
    "        idx = np.where(y == k)[0]\n",
    "        n_class_list.append(len(idx))\n",
    "    class_prob = np.array(n_class_list)/len(y)\n",
    "    np.save(f'{exp_name}/class_prob.npy', np.array(class_prob))\n",
    "\n",
    "    psi_rf_train(x, y, n_projs=500, save_path=exp_name)\n",
    "    \n",
    "    ##############################################################\n",
    "    #\n",
    "    # Compute PSI for all validation and test samples\n",
    "    #\n",
    "    # #############################################################\n",
    "    \n",
    "    thetas = np.load(f'{exp_name}/all_thetas.npy')\n",
    "    class_prob = np.load(f'{exp_name}/class_prob.npy')\n",
    "        \n",
    "    print(f'Computing PSI for all validation samples and for all classes...')\n",
    "    psi_class = []\n",
    "    for k in range(n_classes):\n",
    "        ds_activity = ds_val.batch(cfg['batch_size']).map(lambda x, y: (int_model(x),y)).unbatch()\n",
    "        x = np.array([val.numpy() for val, _ in ds_activity])\n",
    "        psi, pmi_arr = psi_rf_val(x, k, thetas, class_prob, n_projs, save_path=exp_name)\n",
    "        psi_class.append(psi)\n",
    "    np.save(f'{exp_name}/psi_class_{n_projs}_projs_val.npy', np.array(psi_class).T)\n",
    "\n",
    "    print(f'Computing PSI for all test samples and for all classes...')\n",
    "    psi_class = []\n",
    "    for k in range(n_classes):\n",
    "        ds_activity = ds_test.batch(cfg['batch_size']).map(lambda x, y: (int_model(x),y)).unbatch()\n",
    "        x = np.array([val.numpy() for val, _ in ds_activity])\n",
    "        psi, pmi_arr = psi_rf_val(x, k, thetas, class_prob, n_projs, save_path=exp_name)\n",
    "        psi_class.append(psi)\n",
    "    np.save(f'{exp_name}/psi_class_{n_projs}_projs_test.npy', np.array(psi_class).T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "477e1ac7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
